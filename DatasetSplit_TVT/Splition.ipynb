{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b457c77d-696e-4b26-a567-2a1eed8cde4a",
   "metadata": {},
   "source": [
    "# Dataset File - Split\n",
    "# Train/ Validate/ Test ===> 70/20/10"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78c4a6e1-f573-4373-8722-5bbf7ac6904a",
   "metadata": {},
   "source": [
    "# Dataset Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "459e711f-259d-419e-a0e9-dd6f788b4e7c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Tags:\n",
      "PER: 2814\n",
      "LOC: 3576\n",
      "ORG: 4207\n",
      "DATE: 1532\n",
      "MISC: 2775\n",
      "O: 49659\n",
      "\n",
      "BIO Tags:\n",
      "B-PER: 1439\n",
      "I-PER: 1375\n",
      "B-LOC: 2732\n",
      "I-LOC: 844\n",
      "B-ORG: 1956\n",
      "I-ORG: 2251\n",
      "B-DATE: 718\n",
      "I-DATE: 814\n",
      "B-MISC: 2000\n",
      "I-MISC: 775\n",
      "O: 49659\n",
      "\n",
      "Total Sentences: 2534\n",
      "Total Tokens: 64563\n"
     ]
    }
   ],
   "source": [
    "def analyze_ner_tags(text):\n",
    "    # Split the text into lines\n",
    "    lines = text.strip().split('\\n')\n",
    "    \n",
    "    # Initialize counters\n",
    "    tag_counts = {\n",
    "        'TOTAL_TAGS': {},\n",
    "        'BIO_TAGS': {},\n",
    "        'SENTENCES': 0,\n",
    "        'TOTAL_TOKENS': 0  # Count total number of tokens (lines with tags)\n",
    "    }\n",
    "    \n",
    "    # Predefined tag categories\n",
    "    categories = ['PER', 'LOC', 'ORG', 'DATE', 'MISC', 'O']\n",
    "    \n",
    "    # Initialize counts for each category\n",
    "    for cat in categories:\n",
    "        tag_counts['TOTAL_TAGS'][cat] = 0\n",
    "        tag_counts['BIO_TAGS'][f'B-{cat}'] = 0\n",
    "        tag_counts['BIO_TAGS'][f'I-{cat}'] = 0\n",
    "    \n",
    "    # sentence counting\n",
    "    is_sentence_end = False\n",
    "    \n",
    "    # Analyze lines\n",
    "    for i, line in enumerate(lines):\n",
    "        line = line.strip() \n",
    "        if not line:  # Skip empty lines\n",
    "            continue\n",
    "        \n",
    "        # Split the line into tokens\n",
    "        tokens = line.split()\n",
    "        \n",
    "        # Count total tokens (non-empty lines)\n",
    "        tag_counts['TOTAL_TOKENS'] += 1\n",
    "        \n",
    "        # Process each token\n",
    "        for token in tokens:\n",
    "            if '-' in token:\n",
    "                # Split the token to get the tag\n",
    "                tag = token.split('-')[-1]\n",
    "                prefix = token.split('-')[0]\n",
    "                \n",
    "                # Count total tags\n",
    "                if tag in tag_counts['TOTAL_TAGS']:\n",
    "                    tag_counts['TOTAL_TAGS'][tag] += 1\n",
    "                \n",
    "                # Count BIO tags\n",
    "                full_tag = token\n",
    "                if full_tag in tag_counts['BIO_TAGS']:\n",
    "                    tag_counts['BIO_TAGS'][full_tag] += 1\n",
    "            else:\n",
    "                # Count Outside tags\n",
    "                if token == 'O':\n",
    "                    tag_counts['TOTAL_TAGS']['O'] += 1\n",
    "                    tag_counts['BIO_TAGS']['O'] = tag_counts['BIO_TAGS'].get('O', 0) + 1\n",
    "        \n",
    "        # Check if the current line ends a sentence ('. O')\n",
    "        if line == '. O':\n",
    "            is_sentence_end = True\n",
    "        \n",
    "        # Sentence counting: check if the sentence ends after '. O' and followed by an empty line\n",
    "        if is_sentence_end and (i + 1 >= len(lines) or not lines[i + 1].strip()):\n",
    "            tag_counts['SENTENCES'] += 1\n",
    "            is_sentence_end = False  # Reset for the next sentence\n",
    "    \n",
    "    # Remove B-O and I-O from BIO_TAGS\n",
    "    tag_counts['BIO_TAGS'].pop('B-O', None)\n",
    "    tag_counts['BIO_TAGS'].pop('I-O', None)\n",
    "    \n",
    "    return tag_counts\n",
    "\n",
    "# Read text from a file and analyze the tags\n",
    "def analyze_from_file(file_path):\n",
    "    # Read the text data from the file\n",
    "    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "        text = file.read()\n",
    "    \n",
    "    # Analyze the text\n",
    "    results = analyze_ner_tags(text)\n",
    "    \n",
    "    # Print the results\n",
    "    print(\"Total Tags:\")\n",
    "    for tag, count in results['TOTAL_TAGS'].items():\n",
    "        print(f\"{tag}: {count}\")\n",
    "\n",
    "    print(\"\\nBIO Tags:\")\n",
    "    for tag, count in results['BIO_TAGS'].items():\n",
    "        print(f\"{tag}: {count}\")\n",
    "\n",
    "    print(f\"\\nTotal Sentences: {results['SENTENCES']}\")\n",
    "    print(f\"Total Tokens: {results['TOTAL_TOKENS']}\")\n",
    "\n",
    "# Dataset Path and run the fuction (analyze_from_file)\n",
    "file_path = 'AgaCKNER_Dataset.txt'\n",
    "analyze_from_file(file_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "926f2da8-d982-41f9-a5f2-b62f3bc9e4be",
   "metadata": {},
   "source": [
    "# -------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14279f63-b7af-48c1-afe8-662ef024e24e",
   "metadata": {},
   "source": [
    "# Splition - CODE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "230526a4-b463-48d9-8efe-9e8a65f2fa1a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Tags:\n",
      "PER: 2814\n",
      "LOC: 3576\n",
      "ORG: 4207\n",
      "DATE: 1532\n",
      "MISC: 2775\n",
      "O: 49659\n",
      "\n",
      "BIO Tags:\n",
      "B-PER: 1439\n",
      "I-PER: 1375\n",
      "B-LOC: 2732\n",
      "I-LOC: 844\n",
      "B-ORG: 1956\n",
      "I-ORG: 2251\n",
      "B-DATE: 718\n",
      "I-DATE: 814\n",
      "B-MISC: 2000\n",
      "I-MISC: 775\n",
      "O: 49659\n",
      "\n",
      "Total Sentences: 2534\n",
      "Total Tokens: 64563\n",
      "\n",
      "Split Dataset Statistics:\n",
      "Total sentences: 2534\n",
      "Train set: 1773 sentences (70.0%)\n",
      "Validation set: 506 sentences (20.0%)\n",
      "Test set: 255 sentences (10.0%)\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import os\n",
    "\n",
    "def analyze_ner_tags(text):\n",
    "    # Split the text into lines\n",
    "    lines = text.strip().split('\\n')\n",
    "    \n",
    "    # Initialize counters\n",
    "    tag_counts = {\n",
    "        'TOTAL_TAGS': {},\n",
    "        'BIO_TAGS': {},\n",
    "        'SENTENCES': 0,\n",
    "        'TOTAL_TOKENS': 0  # Count total number of tokens (lines with tags)\n",
    "    }\n",
    "    \n",
    "    # Tag categories\n",
    "    categories = ['PER', 'LOC', 'ORG', 'DATE', 'MISC', 'O']\n",
    "    \n",
    "    # Initialize counts for each category\n",
    "    for cat in categories:\n",
    "        tag_counts['TOTAL_TAGS'][cat] = 0\n",
    "        tag_counts['BIO_TAGS'][f'B-{cat}'] = 0\n",
    "        tag_counts['BIO_TAGS'][f'I-{cat}'] = 0\n",
    "    \n",
    "    # Track sentence counting\n",
    "    is_sentence_end = False\n",
    "    \n",
    "    # Analyze lines\n",
    "    for i, line in enumerate(lines):\n",
    "        line = line.strip() \n",
    "        if not line: \n",
    "            continue\n",
    "        \n",
    "        # Split the line into tokens\n",
    "        tokens = line.split()\n",
    "        \n",
    "        # Count total tokens (non-empty lines)\n",
    "        tag_counts['TOTAL_TOKENS'] += 1\n",
    "        \n",
    "        # Process each token\n",
    "        for token in tokens:\n",
    "            if '-' in token:\n",
    "                # Split the token to get the tag\n",
    "                tag = token.split('-')[-1]\n",
    "                prefix = token.split('-')[0]\n",
    "                \n",
    "                # Count total tags\n",
    "                if tag in tag_counts['TOTAL_TAGS']:\n",
    "                    tag_counts['TOTAL_TAGS'][tag] += 1\n",
    "                \n",
    "                # Count BIO tags\n",
    "                full_tag = token\n",
    "                if full_tag in tag_counts['BIO_TAGS']:\n",
    "                    tag_counts['BIO_TAGS'][full_tag] += 1\n",
    "            else:\n",
    "                # Count Outside tags\n",
    "                if token == 'O':\n",
    "                    tag_counts['TOTAL_TAGS']['O'] += 1\n",
    "                    tag_counts['BIO_TAGS']['O'] = tag_counts['BIO_TAGS'].get('O', 0) + 1\n",
    "        \n",
    "        # Check if the current line ends a sentence ('. O')\n",
    "        if line == '. O':\n",
    "            is_sentence_end = True\n",
    "        \n",
    "        # Sentence counting: check if the sentence ends after '. O' and followed by an empty line\n",
    "        if is_sentence_end and (i + 1 >= len(lines) or not lines[i + 1].strip()):\n",
    "            tag_counts['SENTENCES'] += 1\n",
    "            is_sentence_end = False  # Reset for the next sentence\n",
    "    \n",
    "    # Remove B-O and I-O from BIO_TAGS\n",
    "    tag_counts['BIO_TAGS'].pop('B-O', None)\n",
    "    tag_counts['BIO_TAGS'].pop('I-O', None)\n",
    "    \n",
    "    return tag_counts\n",
    "\n",
    "# Read text from a file\n",
    "def analyze_from_file(file_path):\n",
    "    # Read the text data from the file\n",
    "    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "        text = file.read()\n",
    "    \n",
    "    # Analyze the text\n",
    "    results = analyze_ner_tags(text)\n",
    "    \n",
    "    # Print the results\n",
    "    print(\"Total Tags:\")\n",
    "    for tag, count in results['TOTAL_TAGS'].items():\n",
    "        print(f\"{tag}: {count}\")\n",
    "\n",
    "    print(\"\\nBIO Tags:\")\n",
    "    for tag, count in results['BIO_TAGS'].items():\n",
    "        print(f\"{tag}: {count}\")\n",
    "\n",
    "    print(f\"\\nTotal Sentences: {results['SENTENCES']}\")\n",
    "    print(f\"Total Tokens: {results['TOTAL_TOKENS']}\")\n",
    "\n",
    "# Splition\n",
    "def split_ner_dataset(input_file, train_file, val_file, test_file, train_ratio=0.7, val_ratio=0.2, test_ratio=0.1):\n",
    "    \"\"\"\n",
    "    Split a CoNLL format NER dataset into train, validation, and test sets.\n",
    "    Precisely splits sentences and maintains exact formatting.\n",
    "    \"\"\"\n",
    "    # Read the dataset\n",
    "    with open(input_file, 'r', encoding='utf-8') as f:\n",
    "        lines = f.readlines()\n",
    "    \n",
    "    # Extract sentences\n",
    "    sentences = []\n",
    "    current_sentence = []\n",
    "    \n",
    "    for line in lines:\n",
    "        # Strip trailing whitespace\n",
    "        line = line.rstrip()\n",
    "        \n",
    "        # Add line to current sentence\n",
    "        current_sentence.append(line)\n",
    "        \n",
    "        # Check if sentence ends with \". O\"\n",
    "        if line == '. O':\n",
    "            # Add the complete sentence\n",
    "            sentences.append(current_sentence)\n",
    "            current_sentence = []\n",
    "    \n",
    "    # add remaining sentences if exists\n",
    "    if current_sentence:\n",
    "        sentences.append(current_sentence)\n",
    "    \n",
    "    # Shuffle the sentences\n",
    "    random.seed(42)\n",
    "    random.shuffle(sentences)\n",
    "    \n",
    "    # Calculate split indices\n",
    "    total_samples = len(sentences)\n",
    "    train_end = int(total_samples * train_ratio)\n",
    "    val_end = train_end + int(total_samples * val_ratio)\n",
    "    \n",
    "    # Split the dataset\n",
    "    train_set = sentences[:train_end]\n",
    "    val_set = sentences[train_end:val_end]\n",
    "    test_set = sentences[val_end:]\n",
    "    \n",
    "    # Write to files\n",
    "    def write_set(file_path, data_set):\n",
    "        with open(file_path, 'w', encoding='utf-8') as f:\n",
    "            for sentence in data_set:\n",
    "            f.write('\\n'.join(sentence) + '\\n')  # Remove the extra empty line\n",
    "\n",
    "    \n",
    "    write_set(train_file, train_set)\n",
    "    write_set(val_file, val_set)\n",
    "    write_set(test_file, test_set)\n",
    "    \n",
    "    # Print Splition Statistics\n",
    "    print(f\"\\nSplit Dataset Statistics:\")\n",
    "    print(f\"Total sentences: {total_samples}\")\n",
    "    print(f\"Train set: {len(train_set)} sentences ({train_ratio*100}%)\")\n",
    "    print(f\"Validation set: {len(val_set)} sentences ({val_ratio*100}%)\")\n",
    "    print(f\"Test set: {len(test_set)} sentences ({test_ratio*100}%)\")\n",
    "\n",
    "def main():\n",
    "    # Input file path\n",
    "    input_file = 'AgaCKNER_Dataset.txt'\n",
    "    \n",
    "    # Analyze the dataset\n",
    "    analyze_from_file(input_file)\n",
    "    \n",
    "    # check output directorys\n",
    "    os.makedirs('splits', exist_ok=True)\n",
    "    \n",
    "    # Output\n",
    "    train_file = 'splits/train.txt'\n",
    "    val_file = 'splits/val.txt'\n",
    "    test_file = 'splits/test.txt'\n",
    "    \n",
    "    # Split the dataset\n",
    "    split_ner_dataset(input_file, train_file, val_file, test_file)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "efc12c68-8e0a-4242-ad52-36a8e3fcad4f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Total sentences: 2534\n",
      "Train set: 1773 sentences (69.97%)\n",
      "Validation set: 506 sentences (19.97%)\n",
      "Test set: 255 sentences (10.06%)\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import os\n",
    "\n",
    "# Step 1: Read the Input Dataset\n",
    "def read_input_file(input_file):\n",
    "    \"\"\"\n",
    "    Reads the entire dataset from the input file.\n",
    "    Each line consists of a word followed by its NER tag (e.g., word B-LOC),\n",
    "    and sentences are separated by blank lines.\n",
    "    \"\"\"\n",
    "    with open(input_file, 'r', encoding='utf-8') as file:\n",
    "        lines = file.readlines()\n",
    "    return lines\n",
    "\n",
    "\n",
    "# Step 2: Extract Sentences\n",
    "def extract_sentences(lines):\n",
    "    \"\"\"\n",
    "    Groups lines into sentences. A sentence ends when a line equals '. O'.\n",
    "    Accumulates lines for each sentence and stores complete sentences in a list.\n",
    "    \"\"\"\n",
    "    sentences = []\n",
    "    current_sentence = []\n",
    "    \n",
    "    for line in lines:\n",
    "        line = line.rstrip()  # Remove trailing whitespace\n",
    "        if line:  # Skip empty lines\n",
    "            current_sentence.append(line)\n",
    "        if line == '. O':  # Check if this is the end of a sentence\n",
    "            sentences.append(current_sentence)  # Add complete sentence to list\n",
    "            current_sentence = []  # Reset for next sentence\n",
    "    \n",
    "    return sentences\n",
    "\n",
    "\n",
    "# Step 3: Shuffle the Sentences\n",
    "def shuffle_sentences(sentences):\n",
    "    \"\"\"\n",
    "    Randomizes the order of sentences to avoid any bias in the dataset.\n",
    "    A fixed random seed is used for reproducibility.\n",
    "    \"\"\"\n",
    "    random.seed(42)\n",
    "    random.shuffle(sentences)\n",
    "\n",
    "\n",
    "# Step 4: Split the Dataset\n",
    "def split_dataset(sentences, train_ratio=0.7, val_ratio=0.2, test_ratio=0.1):\n",
    "    \"\"\"\n",
    "    Splits the shuffled sentences into training, validation, and test sets.\n",
    "    The split ratios are provided as arguments (default: 70% train, 20% validation, 10% test).\n",
    "    \"\"\"\n",
    "    total_samples = len(sentences)\n",
    "    train_end = int(total_samples * train_ratio)\n",
    "    val_end = train_end + int(total_samples * val_ratio)\n",
    "    \n",
    "    train_set = sentences[:train_end]\n",
    "    val_set = sentences[train_end:val_end]\n",
    "    test_set = sentences[val_end:]\n",
    "    \n",
    "    return train_set, val_set, test_set\n",
    "\n",
    "\n",
    "# Step 5: Write the Subsets to Output Files\n",
    "def write_subsets_to_files(train_set, val_set, test_set, train_file, val_file, test_file):\n",
    "    \"\"\"\n",
    "    Saves each of the subsets (train, validation, test) to their respective output files.\n",
    "    The sentences are written without extra empty lines between sentences.\n",
    "    \"\"\"\n",
    "    def write_set(file_path, data_set):\n",
    "        with open(file_path, 'w', encoding='utf-8') as f:\n",
    "            for sentence in data_set:\n",
    "                f.write('\\n'.join(sentence) + '\\n')  # Join words with newline and write\n",
    "\n",
    "    write_set(train_file, train_set)\n",
    "    write_set(val_file, val_set)\n",
    "    write_set(test_file, test_set)\n",
    "\n",
    "\n",
    "# Step 6: Print Split Statistics\n",
    "def print_split_statistics(sentences, train_set, val_set, test_set):\n",
    "    \"\"\"\n",
    "    Prints the total number of sentences and the count of sentences in the train, validation, and test sets.\n",
    "    Also, displays the corresponding percentages.\n",
    "    \"\"\"\n",
    "    total_samples = len(sentences)\n",
    "    train_samples = len(train_set)\n",
    "    val_samples = len(val_set)\n",
    "    test_samples = len(test_set)\n",
    "\n",
    "    print(f\"\\nTotal sentences: {total_samples}\")\n",
    "    print(f\"Train set: {train_samples} sentences ({(train_samples/total_samples)*100:.2f}%)\")\n",
    "    print(f\"Validation set: {val_samples} sentences ({(val_samples/total_samples)*100:.2f}%)\")\n",
    "    print(f\"Test set: {test_samples} sentences ({(test_samples/total_samples)*100:.2f}%)\")\n",
    "\n",
    "\n",
    "# Main Function: Putting it All Together\n",
    "def main():\n",
    "    # Step 1: Read the Input Dataset\n",
    "    input_file = 'AgaCKNER_Dataset.txt'  # Input file path\n",
    "    lines = read_input_file(input_file)\n",
    "\n",
    "    # Step 2: Extract Sentences\n",
    "    sentences = extract_sentences(lines)\n",
    "\n",
    "    # Step 3: Shuffle the Sentences\n",
    "    shuffle_sentences(sentences)\n",
    "\n",
    "    # Step 4: Split the Dataset\n",
    "    train_set, val_set, test_set = split_dataset(sentences)\n",
    "\n",
    "    # Step 5: Write the Subsets to Output Files\n",
    "    output_dir = 'splits'\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    write_subsets_to_files(train_set, val_set, test_set, \n",
    "                           os.path.join(output_dir, 'train.txt'), \n",
    "                           os.path.join(output_dir, 'val.txt'), \n",
    "                           os.path.join(output_dir, 'test.txt'))\n",
    "\n",
    "    # Step 6: Print Split Statistics\n",
    "    print_split_statistics(sentences, train_set, val_set, test_set)\n",
    "\n",
    "\n",
    "# Run the main function\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c71a0f04-86c5-411c-8f04-2389f125dfa5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
